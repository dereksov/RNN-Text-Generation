{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115394 characters, 65 unique.\n"
     ]
    }
   ],
   "source": [
    "# data I/O\n",
    "data = open('shakespeare.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print ('data has {} characters, {} unique.'.format(data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        y = np.dot(Why, h) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "\n",
    "while True:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "    if n % 1000 == 0:\n",
    "        sample_ix = sample(hprev, inputs[0], 200)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print ('----\\n {} \\n----'.format(txt, ))\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % 1000 == 0: print ('iter {}, loss: {}'.format(n, smooth_loss)) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                  [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                  [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cleared output to not print several hundred pages.\n",
    "\n",
    "reached iter 11155000, loss: 44.498503081985476, before keyboard interrupt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    " vzNSHmEG3r;etQMqoceZVqvsIanUXGA\n",
    "OX$M.jYgcUDCPvAzCdPX jQlMs- bavH!qHEAekR'PF\n",
    ":mNwnDo?J \n",
    ":tnzEA;ygosWrthn;SF?$vFrXM g3HGnvazpNvNoj&z'dhdZUB!;QDzwKy,diriojFRm&;kp;sMcx \n",
    ":erU:leVbk:3Hbh;;KF:E:DwT:rIeH$YKV \n",
    "----\n",
    "iter 0, loss: 104.35968378885154\n",
    "----\n",
    " lrd nve be ca,cMot; tesi: .nut ltolcces bo cor.\n",
    ":\n",
    "Waucus.\n",
    "Whural fnadr ous to 'arus us t etwe,\n",
    "he\n",
    "the bnit,\n",
    "\n",
    "CNSPtoAnnasir thas yid somlme dot sin hthi.!\n",
    "T\n",
    "TwA\n",
    "\n",
    "pCHy thepar mekersorat toi yhiont.\n",
    "Mes  \n",
    "----\n",
    "iter 1000, loss: 87.66641259812343\n",
    "----\n",
    "  ther.\n",
    "re\n",
    "Wonus kI the notowt\n",
    "hane geresde giy ty panl ames\n",
    "CIAme the uat ondcd uiv anern fot thonz to ss\n",
    "he the todened thiter ntd the to psenee heat thawcis nhehand caut-panderealnond math ee.\n",
    "\n",
    "C'L: \n",
    "----\n",
    "iter 2000, loss: 72.3603709993052\n",
    "----\n",
    " es\n",
    "Thave.\n",
    "\n",
    "-ic\n",
    "Balus,\n",
    "\n",
    "Coor potet\n",
    "Theg angiuph tud nond th ath:\n",
    "To rpd, bel cort-d hame nnthe rar bhele note alsthe tor.\n",
    "The we thout, gorpdcoort  af the meet dags gomk an, meesl bee eo acserl the Hes \n",
    "----\n",
    "iter 3000, loss: 64.22507256521415\n",
    "----\n",
    " thal son lised, no es tics on then aso ines ou.\n",
    "Wo wet eat,\n",
    "Morr co fall ok ardt till foby ad senod.\n",
    "\n",
    "Pgaoidst,\n",
    "A'esh ale fleunln fiold,r.\n",
    "S'ep, the huspnos kew'll herid thii,\n",
    "vufime is!\n",
    "San go thee\n",
    "B \n",
    "----\n",
    "iter 4000, loss: 59.987450666118775\n",
    "----\n",
    " o d ao herrpas berrnm\n",
    "Monn:\n",
    "Thus\n",
    "\n",
    "ORICEINIUS:\n",
    "An.\n",
    "\n",
    "'or; yote bEOr coevvy yop, rthot.\n",
    "\n",
    "SINIUS:\n",
    "vawhemense has tho\n",
    "the yor cill.\n",
    "\n",
    "OrF'vif, Me he Loulgmirals\n",
    "INor w seris mowe hafhiynder herm onom\n",
    "Aspiti \n",
    "----\n",
    "iter 5000, loss: 58.57784923447078\n",
    "----\n",
    " pt rereois.\n",
    " it manke.\n",
    "\n",
    "Fyou himasly pene. IC\n",
    "ROS:\n",
    "Fwoth nou'ld pithe Pa mapl dofmti'lt, ppor dsthin baterins\n",
    "Yoy bal comer ba savet\n",
    "in dureif.\n",
    "\n",
    "ANUTUS:\n",
    "Whe, thond hongle yout cesheritherst the the mi \n",
    "----\n",
    "iter 6000, loss: 57.898948863817886\n",
    "----\n",
    " rs edst cukk mate thecf a,,\n",
    "\n",
    "S:u.\n",
    "\n",
    "LUS:\n",
    "Thad doref hamar, wuve awy, kelousagir, the ule.\n",
    "\n",
    "Shivon love bo nomy mes pat Rotine dot mand bo laved kXally thalghe lis,\n",
    "Ga nim wy lome, is to dowetwill how'l \n",
    "----\n",
    "iter 7000, loss: 58.172450012371215\n",
    "----\n",
    " ENTUENEFTURCEMENR\n",
    "TURANUS:\n",
    "Theele wplat aduns ereent.\n",
    "\n",
    "Tha line fintteer ffaccitt,\n",
    "Tulcere, got to thee qunt hat; ure wery upy to think coude hus,\n",
    "I-L\n",
    "qwmecteit ourdee how?\n",
    "\n",
    "\n",
    "SIEREFI\n",
    "Hendenvinbertora' \n",
    "----\n",
    "iter 8000, loss: 57.40812351165954\n",
    "----\n",
    " hit wart sdind thale be on the browh guthsim mou? I I lobas ard to yont as selltitur houresde'st yous, baec\n",
    "Pithe sey cad wt ed thoursor itnerw rie to darseses rdoo he to tistor;\n",
    "And masee $oord be ho\n",
    "\n",
    "----\n",
    "iter 9000, loss: 57.30461917662502\n",
    "\n",
    "----\n",
    "  wleags your uowsente oure thenk;\n",
    "If tath en;\n",
    "A my, How att mnord,\n",
    "Wy uca hervicke Entaagt a ditt anderireng I toll werdes noveen.\n",
    "Mour seerttees noke mave wizl ook;\n",
    "Thine,\n",
    "Hiny sordonse,\n",
    "Ite walche?\n",
    " \n",
    "----\n",
    "iter 10000, loss: 55.846932149898706\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " r the me\n",
    "gothas to sOce am a youl if in abeever to op y to sust hee hip ontser this that a sbus youn breftine?\n",
    "CIame beftharr\n",
    "cormentwood unviens yourd, I I'll vert buther sond, loth maiuris of:\n",
    "And a \n",
    "\n",
    "----\n",
    "iter 100000, loss: 49.27069248455135\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "\n",
    " hought!\n",
    "Lay!\n",
    "Janlle, Cortard prache:\n",
    "In mole.\n",
    "Thy ow cood Hever for where timy's I is ande be, sie, I\n",
    "Whickst natiens,\n",
    "We vinen,\n",
    "Than age wath on gress,\n",
    "That is more not land! and scean hiser did slan \n",
    "\n",
    "----\n",
    "iter 200000, loss: 49.25732551959337\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "\n",
    " sall.\n",
    "\n",
    "BUCVIF\n",
    "MES:\n",
    "But My dronk did youb out thee there of me know's to all to empilt, Of extron.\n",
    "My not foongiech this shoak indiond angyes gizat,\n",
    "Wiest,\n",
    "Marturasbef, I chack your to for do have so a \n",
    "\n",
    "----\n",
    "iter 300000, loss: 48.69393994879168\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "I you yeald age, in rection Jade the tratt,\n",
    "And and notess comke?\n",
    "\n",
    "MENRIINTE:\n",
    "Thinung;\n",
    "Mis, make might on pie.\n",
    "\n",
    "TONTET:\n",
    "Inis your with,\n",
    "Brong?\n",
    "Thou here\n",
    "Swold youip!\n",
    "I will combin sontent you' mintu? \n",
    "\n",
    "----\n",
    "iter 400000, loss: 45.867081976657374\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "\n",
    " hallack prisl sle Hol not\n",
    "Feince'd and me are have long the kids ale contine that sus\n",
    "It mich collule the rimmboacks the stulis, sul you ges'le the she pace\n",
    "To promiag\n",
    "yoyep: not,\n",
    "To seest oa ungrom-- \n",
    "\n",
    "----\n",
    "iter 500000, loss: 45.73411845778061\n",
    "\n",
    "----\n",
    "\n",
    " ow; herar ortect\n",
    "But the soum!\n",
    "A forch of keth good-mill to eyel and shall nitrems.\n",
    "\n",
    "MENCS:\n",
    "The shos-as.\n",
    "\n",
    "Mughten'd in destniligover.\n",
    "Houdn upertles deer to of youreco, not as stis; shane the fremeass \n",
    "\n",
    "----\n",
    "iter 600000, loss: 47.115493566774106\n",
    "\n",
    "----\n",
    "\n",
    " dy in herren\n",
    "that and the me havis\n",
    "atherh sill ports: me, my great:\n",
    "Why of my alitlee lowm:\n",
    "Myseansthed chy,\n",
    "The chmolfages;\n",
    "Ficute't\n",
    "Wemper of are mer,\n",
    "Cifess all frephisenst conghet to lord mett not \n",
    "\n",
    "----\n",
    "iter 700000, loss: 47.56540672142887\n",
    "\n",
    "----\n",
    "\n",
    " y swilf he'se.\n",
    "\n",
    "PETRUCI:\n",
    "Sit,\n",
    "Dhomfens, coth a' vid, my becithen.\n",
    "\n",
    "GRMILIzK OXT:\n",
    "If enter?\n",
    "\n",
    "GRUMINIUSs I like on,\n",
    "Shasy the whicilignteak now the speep gellercely camen:\n",
    "Thel:\n",
    "Your there: Of but of; a \n",
    "\n",
    "----\n",
    "iter 800000, loss: 46.50024399666682\n",
    "\n",
    "----\n",
    "\n",
    "rouse?\n",
    "Here.\n",
    "That greme.\n",
    "And ace chen luch, hadselher tree\n",
    "Goreds-noteted, to me had lask herring 'thirrted to Pe\n",
    "sinest betian are take?\n",
    "\n",
    "CAMISPERT:\n",
    "Alasce, shamather, Vorst hopp,\n",
    "And him.\n",
    "Butiost to \n",
    "\n",
    "----\n",
    "iter 900000, loss: 45.53983959024896"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  whom I mocoret a look on coull aly thit or not.\n",
    "\n",
    "AUFAD:\n",
    "\n",
    "AMANDULESSS II:\n",
    "My to chomeng, and,\n",
    "The bland-feering see-y so treatle as but my ring be tiletut molat count leave seagg, callmeng.\n",
    "\n",
    "GfRCINIUS \n",
    "\n",
    "----\n",
    "iter 1000000, loss: 46.199287022558345\n",
    "\n",
    "----\n",
    "\n",
    " ld a best unforier bet thip it is the woech's\n",
    "Of.\n",
    "\n",
    "WAlrack us parnlaw in it aganters co he good how me Cfelvoning you Efehter were it so blai, good are.\n",
    "He many. Our smabtous you:\n",
    "Or a mate al oh'\n",
    "I f \n",
    "\n",
    "----\n",
    "iter 2000000, loss: 44.525164072047744\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    " AM:\n",
    "They flairs fee lores,\n",
    "But objeeds; has ble I sistor: men the dion good, my freatick dother, her lif, mistrue God us Cost one\n",
    "Orse they you not man where hama an the a he so mithress whoud beef th \n",
    "\n",
    "----\n",
    "iter 3000000, loss: 43.41792691646422\n",
    "\n",
    "----\n",
    "\n",
    "for remand ool homoul the propese.\n",
    "\n",
    "GoNC\n",
    "Four\n",
    "I lath to sobeld in you, in\n",
    "Tlan mas nerrestle tainty,\n",
    "Budion\n",
    "You spead, is do my undore,\n",
    "Think Kater,\n",
    "And and in of do agains being becang take si; hoit \n",
    "\n",
    "----\n",
    "iter 4000000, loss: 44.64289997696883\n",
    "\n",
    "----\n",
    "\n",
    "so feariol's to you knod, I toiss, britherd you be the finsher do never siden, as mest theyer the wades, I stand, it emed! Ed of the bilembred. Lets be take but they with him,\n",
    "Sitrer\n",
    "Or loug be, andep \n",
    "\n",
    "----\n",
    "iter 5000000, loss: 44.01995179623448\n",
    "\n",
    "----\n",
    "\n",
    "ver our he asted conting sisplag mernor streataninn mastollaum it let od menter thingentrierd your as a a stose that herre,\n",
    "That thou most ven is ank bravent and ast I so hat my great and a pide he gr \n",
    "\n",
    "----\n",
    "iter 6000000, loss: 44.45051887593998\n",
    "\n",
    "----\n",
    "\n",
    "ou; and, this nein hald you shall you heard'd Host my gralious been for to of York of high's excoct the heaggove,\n",
    "And a worse in all, calilece best do, you seach Offambob, thine his be and he grese--\n",
    " \n",
    "----\n",
    "iter 7000000, loss: 44.612614795913444\n",
    "\n",
    "----\n",
    "\n",
    "HNET:\n",
    "Kisbay if you!\n",
    "\n",
    "ALIO:\n",
    "Ny gursk's th's ging: fromid.\n",
    "\n",
    "SbAge:\n",
    "Deurth me I keartent puss of this disiese'\n",
    "Nother in't fine, now,\n",
    "Benoth.\n",
    "\n",
    "GLOUCESTER: Civer, their saint afall's thou, fored oow nose \n",
    "\n",
    "----\n",
    "iter 8000000, loss: 44.35128022255806\n",
    "\n",
    "----\n",
    "\n",
    "And ear the mirthy bead an be otherwnibliom;\n",
    "Which thou whesune-fith where tase nike Edalls by farmown, good.\n",
    "\n",
    "LEONTES:\n",
    "Arour unchered oniloasoble\n",
    "Kisse out becand:\n",
    "When is, heaven'd,\n",
    "Antreat, coich  \n",
    "\n",
    "----\n",
    "iter 9000000, loss: 44.79127398306245\n",
    "\n",
    "----\n",
    "\n",
    "ne!\n",
    "O for the good cares to shall iver to yought did\n",
    "\n",
    "Dhepbited in be have but noble the rose, I have!\n",
    "\n",
    "MENENIUS:\n",
    "Sowniul was.\n",
    "\n",
    "AUTILAFNANDKKEN:\n",
    "Ohe came,\n",
    "And iseaty, froe him! I ambenf Ravall.\n",
    "\n",
    "LUCEY \n",
    "\n",
    "----\n",
    "iter 10000000, loss: 44.282232942731675\n",
    "\n",
    "----\n",
    " u peroth hooster, and Caurth. het were thy holld fier up lost he in pad your face thou shall ere the ray.\n",
    "Pesphing. We, hoped tengets him.\n",
    "\n",
    "DUCHESS OF APAUS MIRIA:\n",
    "Noly pakest and neixt they did, reme \n",
    "\n",
    "----\n",
    "iter 11000000, loss: 43.65596742808724"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 512 # size of hidden layer of neurons\n",
    "seq_length = 3 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "\n",
    "while True:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "    if n % 1000 == 0:\n",
    "        sample_ix = sample(hprev, inputs[0], 200)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print ('----\\n {} \\n----'.format(txt, ))\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % 1000 == 0: print ('iter {}, loss: {}'.format(n, smooth_loss)) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                  [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                  [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "    p += seq_length # move data pointer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ndRj'h?wFD  rhH OTRMSgoGwDxytqTgjLoHcS\n",
    "QSW-d$tN:IYD3$jUFVt\n",
    "K\n",
    "iWcpqLOxuNELb.PWv?F'lGAAEyOZLrL.LIYeh\n",
    "MoYDBo.BAglJTgis Xu;ipo&?3hVok'JDdCtpdTZKp,zKFmIj&BezMrrsgGBHiDaBCjPERz;P.lTxJzI;xwfzxXYKHjkg&?!i?Y;T \n",
    "\n",
    "----\n",
    "iter 0, loss: 12.523162139676613\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    " R s  rCthdriNgldmha  npt eniSAAe iotgoeE srhiLel\n",
    " rhiaa,\n",
    "nW \n",
    "hiiK\n",
    "srdhaebcd yee.R snnorwhIe ectrne wy'a ru ,hwd c\n",
    "fse etohr qht;ttyt'K anh\n",
    "E'ol u n\n",
    " th dAo.eeotoeei a,ld lacnutu o sc\n",
    "  reu \n",
    "wet    htr \n",
    "\n",
    "----\n",
    "iter 250000, loss: 10.223829549156331\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  .S nw ,thiEsohnred, o-u tagtOmEo ihre ',aveeBh ahsds no'Wtrl oro ns\n",
    "tfiehsPsaoduroo\n",
    " nhrWisoinl  \n",
    " othElw\n",
    "orrMivtrts dh  rirrHd\n",
    "snwn't iocnesdt rh yldy g T dliR ode ,Sd\n",
    "korwt\n",
    "Reohiia,\n",
    "e\n",
    "  aooJo ebtret \n",
    "  \n",
    "----\n",
    "iter 500000, loss: 10.06532965151112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 3 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " dPuZaxpW,Uw?:IK-aKIMqTkgnsvcZl'jrdXopY3giwXT3yqMM,aN!PlJbQkFmp:quXx!CBPn3.?,g.lKH\n",
      "3q FCSfQvHGOf&SJgyt yLMjsrc:i\n",
      "'kuslUKfcN,EMOZR'YEivblT\n",
      "oAaroUyQzfXDetrBvb:lmXPioeaqKPc?z;G?q.yUnHfm\n",
      "meltUFAuLG;gsUQYoG \n",
      "----\n",
      "iter 0, loss: 12.523161697851636\n",
      "----\n",
      "  asi'd?\n",
      "\n",
      "CSI: snotht onws ife leiginges she rim ver er toas wnethet whouk bee der, oi nont\n",
      "Whe jat nobis ghes bad ther thes iur by as, yobed orom?\n",
      "Fhorlul, angors-tan,\n",
      "\n",
      "CRNOS breed to re alvac: the\n",
      "Na \n",
      "----\n",
      "iter 25000, loss: 6.738765176543091\n",
      "----\n",
      " rmand algo\n",
      "yo rervey cat them hadke whintAret hespis'nt\n",
      "we may\n",
      "Iw fietus fint aime, t'ethes sungre'ns bre tould Codt yuspat, as he vonche geeren the of so lo to weabls\n",
      "And.\n",
      "\n",
      "LANU: I lawrowe me carall  \n",
      "----\n",
      "iter 50000, loss: 6.61930784197637\n",
      "----\n",
      " uwell cugtitht pracleve maclesalatin hend, 'ol stoog.\n",
      "\n",
      "Ther couthley frexrithere's nancheb.\n",
      "\n",
      "QUVENNIZIZHARs anctony\n",
      "OA Gy deenoann.\n",
      "\n",
      "QUEd GLORDE:\n",
      "Racone grouc.\n",
      "Wemears,\n",
      "Withee, wiupw thap and not,\n",
      "Gea \n",
      "----\n",
      "iter 75000, loss: 6.5199665770686615\n",
      "----\n",
      " sost tall, wirter.\n",
      "\n",
      "Dog,\n",
      "Sfreiees, Sayded thou fall thot wivy, msichily werfienwtand mold ship,, andtly deth a fall by wioclshill my tifiuses, remroldfe give womlt wut ars shinn, sworese\n",
      "And tear your \n",
      "----\n",
      "iter 100000, loss: 6.400236923500011\n",
      "----\n",
      " re onmishas?\n",
      "By fof nene lis but droip weis would mo theul winh you your ye whare mitd Cop nayimy;\n",
      "\n",
      "KIIH:\n",
      "If ard I the mis mustrat olle in and and to Bentost of th I so ince gerelan ley witicou that r \n",
      "----\n",
      "iter 125000, loss: 6.2563508672179085\n",
      "----\n",
      " \n",
      "I Thos, 'fore?\n",
      "\n",
      "AnY.\n",
      "I hamd.\n",
      "Wertill thou lit wist Cof\n",
      "I Cingroust sowmer lo oure whe nags;\n",
      "And Cours nintrave on oit ith fiot ring,\n",
      "And tlon, thesest, pound sint wor forle is lo; that of shee\n",
      "Toune  \n",
      "----\n",
      "iter 150000, loss: 6.492319101443601\n",
      "----\n",
      " n'sh-wel;\n",
      "IN may an ald hutidn merpel he ein, ying!\n",
      "\n",
      "JUCIOLI:\n",
      "What say,\n",
      "The bid,\n",
      "eom yoo Laite, Busteanseast, facod nath heall thith friy,\n",
      "Tion of coman Ontek\n",
      "Melef, gare.\n",
      "I\n",
      "ROME:\n",
      "Go sheap dale, you b \n",
      "----\n",
      "iter 175000, loss: 6.334717416516044\n",
      "----\n",
      " n tcate im at the towen ntate, your hath, fuloms nournt ers, griw, Coud sore bese nom.\n",
      "Wheme dar dome tla.\n",
      "At. as thear my fepet.\n",
      "\n",
      "KENT:\n",
      "Ix ticit, the hapis fom wiil's Laild, at Ay To sander theis inc \n",
      "----\n",
      "iter 200000, loss: 6.198967392800545\n",
      "----\n",
      " w, more Hine fery Wom sha;\n",
      "Exse mom ngomy,\n",
      "Buriesus treow hes arde drouttoroee, whous forthred!\n",
      "He mast?\n",
      "\n",
      "QUEEN BENRARGCSEas wis day, thy loves effot my thy brouce?\n",
      "Eners; oue that I wut yee stouth, t \n",
      "----\n",
      "iter 225000, loss: 5.968235160985222\n",
      "----\n",
      " S:\n",
      "I meers, and not his ouvenon:\n",
      "Lover, gome,\n",
      "By grochin this hass ereitud, Gofclhere\n",
      "cind is herd.\n",
      "Thos brikss;\n",
      "He havon elers songy's sosy.\n",
      "Anghanf tom ous nond in hango:\n",
      "Ay.\n",
      "\n",
      "Lomondiant us hig bese \n",
      "----\n",
      "iter 250000, loss: 6.10098641962569\n",
      "----\n",
      " eve have mupring proote musing ce alis mishef!\n",
      "Sy so sino 'A\n",
      "The fnachy:\n",
      "Fithibe gremen to hilrs'esone, you, hasts, evese, mnwere I wod\n",
      "no muspent, it a plave douldrim andst, for fowe cis your our my  \n",
      "----\n",
      "iter 275000, loss: 6.145571294995929\n",
      "----\n",
      " out?\n",
      "\n",
      "DLOUSS:\n",
      "Shere.\n",
      "\n",
      "YLAwn wt keith you rear,,\n",
      "Whe: be,\n",
      "As t'thanses shis besy angt by yey,\n",
      "Thin, fivus dow frowsu, the thas ient.\n",
      "\n",
      "LUCATILO:\n",
      "I 'tllad,\n",
      "Beevigupllakish fous weres is Iwer, cou\n",
      "I blank \n",
      "----\n",
      "iter 300000, loss: 5.875918277084335\n",
      "----\n",
      " me horent was ky fore shayber am Thes dios sha'\n",
      "pel what wa\n",
      "Se the sheld frittly fratrer hisours:\n",
      "Thes dear\n",
      "ASt hath ther wolfed giet vith temorel: draiker a vot it prothal owt ir his griegent:\n",
      "The gi \n",
      "----\n",
      "iter 325000, loss: 6.333783795531347\n",
      "----\n",
      " ry louse with courd ead ay thet\n",
      "And urle'd ence.\n",
      "\n",
      "FRENHIO:\n",
      "Turcher, us rister me not!\n",
      "\n",
      "PO:\n",
      "Io mim?\n",
      "\n",
      "SANTNANGRUTRIO:\n",
      "you blood?\n",
      "\n",
      "TRANIO:\n",
      "Tany in for miss!\n",
      "\n",
      "CRANTIA:\n",
      "We hou,\n",
      "Buy the of maknwelf swere fu \n",
      "----\n",
      "iter 350000, loss: 5.843431126355612\n",
      "----\n",
      "  to quth mee\n",
      "Angust?\n",
      "\n",
      "MESTELUCANNNO:\n",
      "Whis this somal, of whe she and tich\n",
      "Wherns Mur mors mown\n",
      "Whlmst, to ene's deesuld must res I'roeed nock\n",
      "Butst dieg-toy whou forikg of ingrerm\n",
      "Myaldens.\n",
      "\n",
      "GASTION:\n",
      " \n",
      "----\n",
      "iter 375000, loss: 6.056685781933737\n",
      "----\n",
      " Mereces, handimes\n",
      "Thy, be,\n",
      "Wir with:\n",
      "I kinger for be to the food bod your no not\n",
      "That to dost you, bree:\n",
      "The The goustrowt upant dost\n",
      "Lurk tre as I werir, sole?\n",
      "\n",
      "BOMDOMENIUS:\n",
      "Wer's as ditenle, why bra \n",
      "----\n",
      "iter 400000, loss: 5.6971399330690495\n",
      "----\n",
      " ORULNCENIA:\n",
      "At vus alorg.\n",
      "We deagendess me on.\n",
      "An muprer,\n",
      "Agterd pheaks, gorscime bued my hith care goughin then wered-and frouchance,\n",
      "Andle porstief to poreceranow'n on dith comy I lio, to\n",
      "Tithis lir \n",
      "----\n",
      "iter 425000, loss: 6.232605048345243\n",
      "----\n",
      " oud lit you ey sugrood awo thy tor hum trutherain send he take me suceer 'tioim, to' tomp thing fifee biesct: your whor ware that bows wortht the ly who do grackent your veave, Empn heath, we daritn-- \n",
      "----\n",
      "iter 450000, loss: 5.629250465859553\n",
      "----\n",
      " alderssingess'd 'swrinst my no lear; lords?\n",
      "\n",
      "KING:\n",
      "A, it burod hear, I kin kong'd Rew wollmy will wear stantefneld, mealitimy\n",
      "cued letst camnose thee sutltess ant have gries Ristlarted sevess by tithe \n",
      "----\n",
      "iter 475000, loss: 6.305225107930113\n",
      "----\n",
      "  no dare laved they sanderd fore par oly wi laildion I ran oul ma? I read fairen,\n",
      "Lecermast,\n",
      "I.\n",
      "And Grus peacimes trince. We knoots.\n",
      "\n",
      "KING OF YORD\n",
      "The asing wath herswnrold wilcunat,\n",
      "Frosorvenve'lors  \n",
      "----\n",
      "iter 500000, loss: 6.050405095177638\n",
      "----\n",
      " doo the days will.\n",
      "\n",
      "ROCEO:\n",
      "Talece, housathan, for Yould all.\n",
      "\n",
      "ARD IO:\n",
      "Or the sond thoughdos you recomy ber!\n",
      "You ussy?\n",
      "The would bo?\n",
      "And Rear!\n",
      "ye rane.\n",
      "\n",
      "RATHARD:\n",
      "Ont to voulanoniarc-hat manber, is ith  \n",
      "----\n",
      "iter 525000, loss: 6.101251365203511\n",
      "----\n",
      " iid;\n",
      "I breme's the exce he ale, do mowed shis ark\n",
      "aight phor that tomen now concee weo. Beernell natherow, comieimes:\n",
      "Why forse\n",
      "Fork? a' I have me cadtt-stling!\n",
      "\n",
      "HAPF VI:\n",
      "As it new Leas, dearn: coince \n",
      "----\n",
      "iter 550000, loss: 5.6998975373438645\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-c612729dad86>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m   \u001b[1;31m# forward seq_length characters through the net and fetch gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdWxh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhprev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlossFun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhprev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0msmooth_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmooth_loss\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.999\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m25000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'iter {}, loss: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msmooth_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# print progress\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-82e654f329d3>\u001b[0m in \u001b[0;36mlossFun\u001b[1;34m(inputs, targets, hprev)\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mdhnext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWhh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdhraw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdWxh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdparam\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# clip to mitigate exploding gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdWxh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mclip\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mclip\u001b[1;34m(a, a_min, a_max, out, **kwargs)\u001b[0m\n\u001b[0;32m   2095\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2096\u001b[0m     \"\"\"\n\u001b[1;32m-> 2097\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'clip'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_min\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2098\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2099\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_clip\u001b[1;34m(a, min, max, out, casting, **kwargs)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         return _clip_dep_invoke_with_casting(\n\u001b[1;32m--> 141\u001b[1;33m             um.clip, a, min, max, out=out, casting=casting, **kwargs)\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_clip_dep_invoke_with_casting\u001b[1;34m(ufunc, out, casting, *args, **kwargs)\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;31m# try to deal with broken casting rules\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_exceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_UFuncOutputCastingError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[1;31m# Numpy 1.17.0, 2019-02-24\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "\n",
    "while True:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "    if n % 25000 == 0:\n",
    "        sample_ix = sample(hprev, inputs[0], 200)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print ('----\\n {} \\n----'.format(txt, ))\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % 25000 == 0: print ('iter {}, loss: {}'.format(n, smooth_loss)) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                  [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                  [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  wncadr\n",
    "Aigiin we sen Whorh wund ber freers:\n",
    "Ye ser yhishe on pet,\n",
    "Id tord ar an ver neirecerocherorgsn, mich bege wicingrtof; soult'd thuol ro the thand yord ans'kss sil un sonl fort tong path cow,-b \n",
    "\n",
    "----\n",
    "iter 25000, loss: 62.89360021621069\n",
    "\n",
    "----\n",
    "\n",
    " :\n",
    "Nesuuve, and, And ir peing thy I dove grom, shese\n",
    "Yo kave en me; fumirh\n",
    "\n",
    "LEI ISINIU Lo st kied,\n",
    "Foneam niwk su of theet hil in\n",
    "I pleey teto o to tharm tis ens\n",
    "\n",
    "MIU\n",
    "HAGINUCINISENIUS Thans pey is with \n",
    "\n",
    "----\n",
    "iter 50000, loss: 59.6063652664936\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    " tho bood hare ige unoaly o pathilenes,\n",
    ": o then!\n",
    "\n",
    "FLONE:\n",
    "The, quye wey is shim'd chomi'n it; at the at nhofshens thalle Bren yorde asd\n",
    "And the fere, at ceas gis that dorers cyot her.\n",
    "You spit mnt se f \n",
    "\n",
    "----\n",
    "iter 75000, loss: 57.986586951949285\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "e int.\n",
    "shat, my toll.\n",
    "\n",
    "BUNETI:\n",
    "Thap at ut vomes:\n",
    "yor I hel them in me dow ald bren sellisden you, starngt ef,' con!\n",
    "Wow Rot sthe duker wy, dovu mupnts'grases, spon:\n",
    "Pve:\n",
    "And mand in\n",
    "Bithan:\n",
    "Sigome ous \n",
    "----\n",
    "iter 100000, loss: 55.983037858774246"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
